# Changelog

## [2.7.0] - 2025-09-07 - Real-Time Judge Display & System Voices V2

### ğŸ‘ï¸ **Real-Time Judge Transparency**
- **Live Judge Display**: Individual judge scores shown in real-time evolution progress
  - `âœ… Iteration 2: add_fewshot | score 0.366 | judges: llama: 0.80, scout: 0.75 (tie-breaker)`
- **Judge Information Streaming**: Backend now includes judge metadata in SSE events
- **Tie-Breaker Indicators**: Clear visual indication when tie-breaker judges were used
- **Model Name Shortening**: Simplified judge model names for clean UI display

### ğŸ­ **System Voices V2 (Experimental)**
- **8 Specialized AI Personas**: Task-aware system voice selection with weighted distribution
  - **Engineer**: Minimal, directly usable code/config  
  - **Analyst**: Brief reasoning steps with assumption validation
  - **Optimizer**: Creative alternatives with tradeoff comparisons
  - **Specialist**: Detail-oriented with edge case coverage
  - **Architect**: Robust, extensible system design
  - **Product Strategist**: User value and business impact focus
  - **Experimenter**: Rapid prototyping and small tests
  - **Skeptic**: Rigorous assumption testing and failure analysis
- **Task-Class Weighting**: Different personas favored based on task type (code, analysis, writing, business, research)
- **Feature Flag Control**: `FF_SYSTEMS_V2` enables/disables the enhanced system voice selection

### ğŸ”§ **Technical Implementation**
- **Streaming Enhancement**: Added `judge_info` field to iteration events with judge scores and tie-breaker status
- **UI Processing**: JavaScript formatting of judge information for status line display
- **Weighted Selection**: Random selection from weighted persona pools based on task classification
- **Backward Compatibility**: All existing functionality preserved when feature flags disabled

### ğŸ› ï¸ **Configuration**
- **New Feature Flag**: `FF_SYSTEMS_V2=1` to enable advanced system voices
- **Judge Metadata**: Automatic extraction from two-judge evaluation results
- **Error Handling**: Graceful fallback when judge information unavailable

---

## [2.6.0] - 2025-09-06 - Advanced Two-Judge AI Scoring System

### ğŸ§  Revolutionary AI Evaluation Architecture
- **Two-Judge System**: Independent evaluation by two different Groq models for robust scoring
- **Automatic Tie-Breaker**: When judges disagree significantly (â‰¥0.3), third judge reviews both evaluations and makes final decision
- **Smart Model Rotation**: Intelligent distribution across 10 cutting-edge models with usage tracking for fairness
- **90/10 AI/Semantic Weighting**: Heavily favor AI judgment over topical similarity for accuracy assessment

### ğŸ¯ Model Pool Excellence
**10 State-of-the-Art Models** with specialized capabilities:
- `llama-3.3-70b-versatile` - Advanced reasoning and analysis
- `openai/gpt-oss-120b` - Large-scale language understanding
- `openai/gpt-oss-20b` - Efficient high-quality evaluation  
- `llama-3.1-8b-instant` - Fast, reliable scoring
- `groq/compound` - Multi-faceted comprehensive analysis
- `groq/compound-mini` - Lightweight efficient evaluation
- `meta-llama/llama-4-maverick-17b-128e-instruct` - Latest instruction-following
- `meta-llama/llama-4-scout-17b-16e-instruct` - Exploration-focused evaluation
- `qwen/qwen3-32b` - Advanced multilingual capabilities
- `moonshotai/kimi-k2-instruct` - Specialized instruction understanding

### ğŸ”¬ Technical Implementation
- **Weighted Model Selection**: Inverse frequency weighting ensures even distribution across all models
- **Disagreement Detection**: Automatically triggers tie-breaker when initial judges differ by 30%+
- **Comprehensive Metadata**: Full evaluation breakdown with individual judge scores, reasoning, and decisions
- **Graceful Fallbacks**: Robust error handling with semantic similarity backup when AI judges unavailable
- **Transparent Process**: Detailed logging of which models were used, their individual scores, and final decision rationale

### ğŸ“Š Evaluation Criteria
**AI Judges Score On:**
- **Accuracy & Correctness**: Factual accuracy and logical reasoning
- **Completeness & Thoroughness**: Coverage of all task requirements
- **Clarity & Coherence**: Communication effectiveness and structure
- **Relevance to Task**: Alignment with specific requirements
- **Practical Usefulness**: Real-world applicability and value

### ğŸ›  Integration Benefits
- **Enhanced Outcome Rewards**: More accurate quality assessment leads to better evolution guidance
- **Reduced Noise**: Two-judge consensus eliminates outlier evaluations
- **Better UCB Bandit Performance**: More reliable scores improve operator selection accuracy  
- **Detailed Analytics**: Rich metadata enables deeper analysis of what makes responses high-quality

### âš™ï¸ Configuration
- **Disagreement Threshold**: 0.3 (configurable) triggers tie-breaker evaluation
- **Weighting**: 90% AI judgment, 10% semantic similarity (optimized through testing)
- **Model Selection**: 2-3 models per evaluation with intelligent rotation
- **Fallback Strategy**: Graceful degradation to semantic-only scoring if needed

---

## [2.5.0] - 2025-09-06 - Human-in-the-Loop Rating Enablement

### âœ… Whatâ€™s New
- SSE iteration events now include `variant_id` and `output` preview to power the rating panel.
- Added endpoint `GET /api/meta/variants/{variant_id}` to fetch full model outputs for review.
- Rating API stabilized: `POST /api/meta/rate` accepts `{ variant_id, human_score (0â€“1), feedback }` and stores 1â€“10 scores.
- UI Quick Test fixes: reads `response` field for chat; session autoâ€‘creation ensures valid `session_id`.
- Advanced settings wired: strategy selector (UCB1 vs epsilonâ€‘greedy), RAGâ€‘K control; iteration max aligned to 24.

### Impact
- Human rating is functional during evolution; submissions are persisted and associated to exact variants.
- Users can inspect the actual outputs generated via the new variant endpoint.
- Advanced settings are now effective and reflected in backend behavior.

### Notes
- Stream chat in the UI parses `{ "token": "..." }` chunks; optional to stop on `{ "done": true }`.
- Îµ slider is meaningful when `epsilon_greedy` is selected; UCB1 remains the default.

## [2.4.0] - 2025-09-06 - Enhanced Operator Exploration

### ğŸ¯ Improved Operator Selection & Diversity
- **Increased Exploration**: Raised epsilon from 0.3 to 0.6 for more operator experimentation
- **Forced Initial Exploration**: Ensures every operator gets tried at least once before exploitation begins
- **UCB Algorithm**: Added Upper Confidence Bound as alternative to epsilon-greedy selection
- **Selection Algorithm Choice**: New `bandit_algorithm` parameter supports "epsilon_greedy" or "ucb"

### ğŸ”¬ Technical Implementation
- **Enhanced EpsilonGreedy**: Modified to prioritize untried operators first, then apply epsilon-greedy logic
- **New UCB Class**: Implements Upper Confidence Bound with confidence interval calculation
- **Algorithm Flexibility**: Runtime selection between exploration strategies via configuration
- **Improved Coverage**: Systematic exploration ensures all 11 available operators get evaluated

### ğŸ“Š Problem Solved
- **Previous Issue**: Only 7 out of 11 operators were being used due to early exploitation
- **Root Cause**: Îµ=0.3 (30% exploration) led to premature convergence on successful operators
- **Solution Impact**: Now guarantees all operators (change_system, raise_temp, add_fewshot, use_groq, etc.) get tried
- **Verification**: Testing confirms 100% operator coverage with both improved algorithms

### ğŸ¯ **Current Results - Improved Diversity Evidence**
**Recent Runs (Post-Implementation)** show previously unused operators now active:
- **âœ… use_groq** - Successfully tried in run #20 (previously never used)
- **âœ… add_fewshot** - Now appearing in run #22 (previously rare)
- **âœ… raise_temp** - Multiple uses in run #22 (previously avoided)
- **âœ… change_system** - Successfully tried in run #22 (previously never used)
- **âœ… inject_memory** - Active usage in recent runs

**Before vs After Comparison:**
- **Old Behavior**: Converged quickly on ~3 operators (toggle_web, lower_temp, lower_top_k)
- **New Behavior**: Systematic exploration of all 11 operators with forced initial trials

### âš™ï¸ Configuration Changes
- **Default Epsilon**: Increased from 0.3 to 0.6 in `.env` and `config.py`
- **New Parameter**: `bandit_algorithm` in `meta_run()` function
- **Backward Compatibility**: Existing calls default to epsilon-greedy behavior
- **Environment Override**: `META_DEFAULT_EPS=0.6` can be customized via environment

---

## [2.3.0] - 2025-09-06 - Long-Term Analytics Dashboard

### ğŸ“ˆ Analytics & Learning Insights
- **Comprehensive Analytics API**: New `/api/meta/analytics` endpoint providing system-wide performance metrics
- **Evolution Progress Tracking**: Shows score progression over time with rolling averages
- **Operator Performance Analysis**: Visual charts of best-performing strategies (lower_temp, lower_top_k leading)
- **Task Type Comparison**: Performance breakdown by task class (code, research, business, etc.)
- **Improvement Trends**: Early vs recent performance comparison showing long-term learning
- **System Statistics**: Total runs, timespan, and overall performance metrics

### ğŸ¨ Interactive Dashboard UI
- **Collapsible Analytics Section**: "ğŸ“ˆ Long-Term Analytics & Improvement" in UI
- **Real-time Data Loading**: Click "Load Analytics" button for latest insights
- **Visual Performance Charts**: Progress bars and trend displays for easy interpretation
- **Responsive Grid Layout**: Organized display of metrics, operators, and task performance
- **Error Handling**: Graceful degradation with user feedback

### ğŸ”§ Technical Implementation
- **Infinite Value Handling**: Robust JSON serialization with -inf/inf cleanup
- **Rolling Window Analysis**: 3-run rolling averages for trend smoothing
- **Database Optimization**: Efficient queries with proper filtering and indexing
- **Frontend Integration**: JavaScript functions with global scope registration
- **Performance Metrics**: Average execution times and reward calculations

### ğŸ“Š Key Insights Revealed
- **Top Operators**: `lower_temp` (22.96 reward), `lower_top_k` (21.03 reward)
- **Strategy Learning**: System identifies best-performing approaches over time  
- **Task Adaptation**: Performance varies by task type, enabling targeted optimization
- **Continuous Improvement**: Historical data proves system evolution across runs

---

## [2.2.0] - 2025-09-06 - M1 Upgrades (Trajectory, Rewards, Masks, Gating)

### âœ¨ New (Flagged ON by default)
- Trajectory logging per run: writes `runs/{ts}/trajectory.json` with perâ€‘iteration op/engine/time/score/reward.
- Process + cost reward blending: bandit reward = Î±Â·(scoreâˆ’baseline) + Î²Â·Î”process âˆ’ Î³Â·time_ms (envâ€‘tunable weights).
- Operator masks per task: optional `storage/operator_masks.json` (task_class â†’ framework_mask/operators allowlist).
- Eval suite + gating: safety probes at end of run; writes `runs/{ts}/eval.json` and adds `eval` to results.

### ğŸ›  Defaults & Wiring
- Meta defaults updated to N=12, Îµ=0.3 (env overridable).
- SSE event alignment: frontend handles `iter`, `judge`, `done` events from backend.
- Async runs: UI uses `POST /api/meta/run_async` and streams `/api/meta/stream`.
- Stats init fix: `/api/meta/stats` initializes DB schema to avoid early 500s.
- No appâ€‘side token caps for Chat/Meta (models control length).
- Groq picker filters out nonâ€‘chat models (tts/whisper/embed).

### âš™ï¸ Feature Flags (env)
- `FF_TRAJECTORY_LOG`, `FF_PROCESS_COST_REWARD`, `FF_OPERATOR_MASKS`, `FF_EVAL_GATE` (all enabled by default).
- Reward weights: `REWARD_ALPHA`, `REWARD_BETA_PROCESS`, `REWARD_GAMMA_COST`.
- Meta defaults: `META_DEFAULT_N`, `META_DEFAULT_EPS`.

---

## [2.1.0] - 2025-09-06 - Human Rating System

### ğŸ§‘â€âš–ï¸ Human-in-the-Loop Feedback
- **Interactive Rating Panel**: Rate AI responses during evolution with 1-10 scale or thumbs up/down
- **Real-time Rating Interface**: Shows after each iteration response for immediate feedback
- **Feedback Collection**: Optional text feedback alongside numerical ratings
- **Database Integration**: All ratings stored in `human_ratings` table with variant linkage
- **API Endpoint**: `/api/meta/rate` for programmatic rating submission
- **Validation**: Proper input validation (1-10 scores, valid variant IDs)

### ğŸ”§ Technical Implementation
- **New Database Table**: `human_ratings(id, variant_id, human_score, feedback, created_at)`
- **Request Model**: `HumanRatingRequest` with validation constraints
- **Store Function**: `save_human_rating()` for persistent storage
- **UI Integration**: Rating panel appears automatically during evolution
- **Error Handling**: Graceful validation and user feedback

### ğŸ“± User Experience
- **Seamless Workflow**: Rating panel appears after each AI response
- **Multiple Rating Methods**: Quick thumbs up/down or detailed 1-10 scale
- **Visual Feedback**: Real-time rating submission status
- **Non-intrusive Design**: Collapsible panel maintains clean interface

---

## [2.0.0] - 2025-09-06 - Human-Centered UI Redesign

### ğŸ¯ Major UI Overhaul
- **Complete interface redesign** using human-centered design principles
- **Single primary action**: Focused "ğŸš€ Start Evolution" workflow
- **Natural language prompts**: "What should the AI get better at?" instead of technical jargon
- **Progressive disclosure**: Advanced settings collapsed by default
- **Task-oriented design**: Dropdown for Code, Analysis, Writing, Business, etc.

### ğŸ“Š Real-time Progress & Streaming
- **Visual progress bars** with real-time completion percentages
- **Live step tracking**: "ğŸ”„ Iteration 2: Trying toggle_web" status updates
- **Streaming output display**: Watch AI responses as they're generated
- **Connection status**: "ğŸ“¡ Connected to evolution stream" feedback
- **Comprehensive logging**: Debug info in browser console (F12)

### ğŸ¨ Improved User Experience
- **Collapsible sections** for clean interface:
  - ğŸ’¬ Quick Test Current AI
  - âš™ï¸ Advanced Evolution Settings
  - ğŸ“Š View Evolution History
- **Health monitoring**: Auto-updating Ollama/Groq status badges
- **Error handling**: Clear error messages with recovery suggestions
- **Mobile-friendly**: Responsive design for all devices
- **Glassmorphism theme**: Modern dark UI with backdrop blur effects

### ğŸ”§ Technical Improvements
- **Fixed meta-evolution freezing bug**: Runs now always complete properly
- **Enhanced health endpoints**: Added `/api/health/ollama` endpoint
- **Better error handling**: Graceful degradation with user feedback
- **JavaScript fixes**: Corrected static file paths and loading issues
- **Console debugging**: Detailed logging for troubleshooting

### ğŸš€ New User Flow
1. Enter task description (natural language)
2. Select task type and number of iterations
3. Click "Start Evolution" button
4. Watch real-time progress with streaming updates
5. View results with improvement metrics and best strategies

### ğŸ› ï¸ Bug Fixes
- Fixed health badges stuck on "checking..." status
- Resolved JavaScript loading issues (404 errors)
- Fixed meta-evolution runs not completing (`finished_at` null bug)
- **Fixed JavaScript function scope issue**: Resolved "startEvolution is not a function" error by explicitly attaching functions to window object
- **Complete JavaScript functionality restoration**: Added missing `quickTest`, `streamTest`, `loadEvolutionHistory` functions with proper global scope
- Improved streaming connection handling with timeout management
- Enhanced error reporting with detailed stack traces

### ğŸ“ˆ Performance Improvements
- **Streaming progress updates**: Real-time feedback during evolution
- **Connection pooling**: Better resource management for long-running operations
- **Caching improvements**: Faster static file serving
- **Background processing**: Non-blocking evolution runs

---

## [1.x.x] - Previous Versions

### Legacy Features (Still Available)
- Multi-engine support (Ollama + Groq)
- Epsilon-greedy bandit optimization
- Recipe persistence and analytics
- Memory system with FAISS vector search
- RAG integration with document indexing  
- Web search capabilities
- Real-time streaming with Server-Sent Events
- Judge mode for comparative evaluation

### Migration Notes
- All legacy functionality preserved in hidden compatibility layer
- Old API endpoints remain functional
- Previous configurations still work
- Technical users can access advanced features through collapsible sections
## [2.6.0] - 2025-09-07 - Phase 4 (AlphaEvolveâ€‘lite) Completed

### âœ… Loop Orchestration & Safety
- Autoâ€‘invocation behind `FF_CODE_LOOP` after meta runs; nonâ€‘blocking with queue + global lock and idempotency per `source_run_id`.
- Rate limit and hard timeout with configurable envs; `live` and `dry_run` modes.

### ğŸ§ª Tests & Guardrails
- Added unit tests for reward math, UCB behavior, loop controls, and artifact integrity.
- Enforced allowlist with tiny JSON tuning patches; autoâ€‘revert on failure; diff snippet and optional git hash recorded.

### ğŸ Golden Set
- Expanded to â‰¥3â€“5 items per task type (â‰ˆ30+ total); deterministic seeds; web off; rag_k pinned.
- Endpoints: `GET /api/golden/list`, `POST /api/golden/run`; artifacts written to `runs/<ts>/golden_kpis.json`.

### ğŸ“Š Analytics Deepening
- Judges: tieâ€‘breaker rate and eval latency p50/p90; operators: coverage & mean total_reward; Golden trends by task_type; thresholds surfaced.
- UI panels: Judges, Operators, Golden Set, Costs.

### ğŸ›¡ Policy & Rewards
- Localâ€‘only generation (engine="ollama"); Groq used for evaluation only (twoâ€‘judge + tieâ€‘breaker) with metadata.
- Bandit uses `total_reward`; artifacts include `reward_breakdown` and `bandit_state`.
